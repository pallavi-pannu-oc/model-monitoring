{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "import sys,json\n",
    "!{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --user >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import kfp\n",
    "from kfp import components\n",
    "from dkube.sdk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkube_preprocessing_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/preprocess/component.yaml\")\n",
    "dkube_training_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/training/component.yaml\")\n",
    "dkube_serving_op  = components.load_component_from_file(\"/mnt/dkube/pipeline/components/serving/component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DkubeApi(token=os.getenv(\"DKUBE_USER_ACCESS_TOKEN\",TOKEN))\n",
    "client = kfp.Client(host=os.getenv(\"KF_PIPELINES_ENDPOINT\"),existing_token=os.getenv(\"DKUBE_USER_ACCESS_TOKEN\",TOKEN),namespace=DKUBEUSERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_program = 'titanic'\n",
    "if (DATA_SOURCE == 'local' or DATA_SOURCE == 'aws_s3') and INPUT_TRAIN_TYPE == 'retraining':\n",
    "    input_training_dataset = mm_name+'-groundtruth'\n",
    "elif DATA_SOURCE == 'sql':\n",
    "    input_training_dataset = 'titanic-data-sql'\n",
    "else:\n",
    "    input_training_dataset = 'titanic-data'\n",
    "\n",
    "## Preprocessing stage inputs\n",
    "preprocessing_script =f\"pip3 install pymysql --user;python titanic/preprocessing.py --data_source {DATA_SOURCE} --train_type {INPUT_TRAIN_TYPE} --monitor_name {MONITOR_NAME} --user {DKUBEUSERNAME}\"\n",
    "input_dataset_mount = ['/data']\n",
    "output_dataset = 'titanic-training-data'\n",
    "output_mount_path = ['/train-data']\n",
    "\n",
    "## Training stage inputs\n",
    "training_script = f\"python titanic/training.py --data_source {DATA_SOURCE}\"\n",
    "model_name = 'titanic-model'\n",
    "output_model_mount = \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='dkube-titanic-pl',\n",
    "    description='sample titanic pipeline with dkube components'\n",
    ")\n",
    "def titanic_pipeline(token):\n",
    "    \n",
    "    preprocessing = dkube_preprocessing_op(\n",
    "                                    auth_token=str(token),\n",
    "                                    container=json.dumps({\"image\":\"docker.io/ocdr/dkube-datascience-tf-cpu:v2.0.0-3\"}),\n",
    "                                    program=str(training_program),\n",
    "                                    datasets = json.dumps([str(input_training_dataset)]),\n",
    "                                    input_dataset_mounts = json.dumps(input_dataset_mount),\n",
    "                                    run_script=str(preprocessing_script),\n",
    "                                    outputs=json.dumps([str(output_dataset)]),\n",
    "                                    output_mounts=json.dumps(output_mount_path)).set_display_name(\"data-generation\")\n",
    "    \n",
    "    train       = dkube_training_op(auth_token = token,\n",
    "                                    container = json.dumps({\"image\":\"docker.io/ocdr/dkube-datascience-tf-cpu:v2.0.0-3\"}),\n",
    "                                    framework=\"tensorflow\", version=\"2.0.0\",\n",
    "                                    program=str(training_program), \n",
    "                                    run_script=str(training_script),\n",
    "                                    datasets=json.dumps([str(output_dataset)]), outputs=json.dumps([str(model_name)]),\n",
    "                                    input_dataset_mounts=json.dumps(output_mount_path),\n",
    "                                    output_mounts=json.dumps([str(output_model_mount)]),\n",
    "                                    envs='[{\"EPOCHS\": \"1\"}]').after(preprocessing)\n",
    "    \n",
    "    serving     = dkube_serving_op(auth_token = token,model = train.outputs['artifact'], device='cpu', \n",
    "                                    serving_image=json.dumps({\"image\":\"ocdr/tensorflowserver:2.0.0\"}),\n",
    "                                    transformer_image=json.dumps({\"image\":\"ocdr/dkube-datascience-tf-cpu:v2.0.0\"}),\n",
    "                                    transformer_project=str(training_program),\n",
    "                                    transformer_code='titanic/transformer.py').after(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_run_from_pipeline_func(titanic_pipeline, arguments={'token':TOKEN})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
